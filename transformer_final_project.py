# -*- coding: utf-8 -*-
"""Transformer Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_bDp2__YBFXD4XEN0rNwEGIw95ysRme

# Question - Answering pipeline

## Environment Setup

install packages
"""


"""import packages"""

import dspy
from dspy.teleprompt import BootstrapFewShot
from dspy.evaluate import answer_exact_match, answer_passage_match
from dspy import Example
from dspy.teleprompt import BootstrapFewShot
from dspy import Example
from dspy.evaluate.evaluate import Evaluate
from dspy.evaluate import Evaluate
import random
import re


from datasets import load_dataset

import json
import pandas as pd


from dspy.retrieve.chromadb_rm import ChromadbRM
from transformers import LongformerTokenizer, LongformerModel
from dspy.teleprompt import BootstrapFewShot
import chromadb
from chromadb import Client
import torch
import numpy as np
from FlagEmbedding import BGEM3FlagModel
import tqdm
from sklearn.preprocessing import normalize


from collections import Counter


from huggingface_hub import login
from huggingface_hub import snapshot_download, login

login("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")
token = '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'

torch.cuda.is_available()

"""only for running ollama in google colab

### Set up LM
"""



"""### Set up RM

Embed dataset with M3 Embedding and Save the ebedded dataset to ChromaDB
"""

# Initialize the M3 model (BGEM3FlagModel)
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)

# Initialize the M3 model
def initialize_m3_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")
    return BGEM3FlagModel('BAAI/bge-m3', use_fp16=True, device=device)

# Embed texts
def embed_texts_m3(texts, model, batch_size=16, max_length=8192):
    embedded_texts = []

    def batchify(data, batch_size):
        return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]

    for batch in batchify(texts, batch_size):
        embeddings = model.encode(batch, batch_size=batch_size, max_length=max_length)['dense_vecs']
        embedded_texts.extend(embeddings)

    return embedded_texts

squad_v2 = load_dataset('squad_v2')
train_data = squad_v2['train']
validation_data = squad_v2['validation']
print(f"Number of training examples: {len(train_data)}")
print(f"Number of validation examples: {len(validation_data)}")

from datasets import load_dataset

# Load the squad_v2 dataset
squad_v2 = load_dataset('squad_v2')

# Extract unique titles from the training data
unique_titles = set(squad_v2['train']['title'])

# Print the unique titles
print(unique_titles)

# Titles to keep (related to travel, hospitality, and attractions)
titles_to_keep = [
    "St._John's,_Newfoundland_and_Labrador", "Charleston,_South_Carolina", "Kathmandu",
    "Saint_Barthélemy", "Brasília", "Detroit", "Ann_Arbor,_Michigan", "Norfolk_Island",
    "Atlantic_City,_New_Jersey", "Richmond,_Virginia", "Valencia", "Philadelphia",
    "London", "Paris", "Southampton", "Melbourne", "Montevideo", "Buckingham_Palace",
    "Egypt", "Miami", "New_Haven,_Connecticut", "Tucson,_Arizona", "Mexico_City",
    "San_Diego", "Seattle", "Santa_Monica,_California", "Florida", "Tennessee",
    "Alaska", "Oklahoma", "North_Carolina", "Montana", "Universal_Studios",
    "Turner_Classic_Movies", "Guam", "Cyprus", "Chihuahua_(state)", "Samoa",
    "Alsace", "Buckingham_Palace"
]

# Filter the dataset to include only rows with the specified titles
squad_data = squad_v2['train'].filter(lambda example: example['title'] in titles_to_keep)

# Display the number of rows in the filtered dataset
len(squad_data)

"""##### Drop the duplicated passages before embedding"""

contexts = squad_data['context']

unique_contexts = list(set(contexts))

print(f"Original number of contexts: {len(contexts)}")
print(f"Number of unique contexts: {len(unique_contexts)}")

squad_data['question'][31:45]

# Initialize ChromaDB client and collection
# Path to store ChromaDB
db_path = "./chromadb1"
client = chromadb.PersistentClient(path=db_path)
collection = client.get_or_create_collection(name="squad_data", metadata={"hnsw:space": "cosine"})

# Maintain unique contexts and their corresponding IDs
context_to_id = {}
for context, doc_id in zip(squad_data['context'], squad_data['id']):
    if context not in context_to_id:
        context_to_id[context] = doc_id

# Extract unique contexts and their corresponding IDs
documents = list(context_to_id.keys())
ids = list(context_to_id.values())

# Generate embeddings for the contexts using the M3 embedding function
model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
embeddings = embed_texts_m3(documents, model, batch_size=16, max_length=8192)

# Generate metadata for each document
metadata = [{"source": "SQuAD", "category": "context", "index": i} for i in range(len(documents))]
# Split data into smaller batches to meet the maximum batch size limit of 5461
batch_size = 5461
for i in range(0, len(documents), batch_size):
    # Batch documents, embeddings, ids, and metadata
    batch_documents = documents[i:i + batch_size]
    batch_embeddings = embeddings[i:i + batch_size]
    batch_ids = ids[i:i + batch_size]
    batch_metadata = metadata[i:i + batch_size]

    # Add the current batch to ChromaDB
    collection.add(
        documents=batch_documents,
        embeddings=[embedding.tolist() for embedding in batch_embeddings],  # Convert embeddings to list of lists
        ids=batch_ids,
        metadatas=batch_metadata
    )

# Initialize the retriever model with the M3 embedding function
retriever_model = ChromadbRM(
    collection_name="squad_data",
    persist_directory=db_path,
    embedding_function=m3_embedding_function,
    k=5
)

"""##### Queries Testing"""

def retrieve_and_display_results(retriever_model, query, top_n=5):
    """
    Function to retrieve and display results along with similarity scores.

    Args:
        retriever_model: The initialized retriever model for querying the ChromaDB.
        query (str): The query text to search in the retriever model.
        top_n (int): Number of top documents to display (default is 5).
    """
    # Perform the query using the retriever model
    results = retriever_model(query)

    # Sort results by similarity score in descending order (highest to lowest)
    results = sorted(results, key=lambda x: x['score'])

    # Display retrieved results along with similarity scores
    for idx, result in enumerate(results[:top_n], start=1):
        similarity_score = result['score']
        long_text = result['long_text']
        print(f"Document {idx}:")
        print(f"Cosine Distance: {similarity_score:.4f}")
        print(f"Content: {long_text}")
        print()

# Example query to retrieve and display results
query = "What is another name for Universal Studios Inc.?"
retrieve_and_display_results(retriever_model, query, top_n=5)

# Test different queries
query_1 = "Who were Laemmle's business partners in the Yankee Film Company?"

retrieve_and_display_results(retriever_model, query_1, top_n=5)

# Check the shape of the embeddings
print("Shape of embeddings:", np.array(embeddings).shape)

"""### Loading Generator: Mistral - 7b"""

# Commented out IPython magic to ensure Python compatibility.

# %load_ext colabxterm
# %xterm
#  curl https://ollama.ai/install.sh | sh
#  ollama serve & ollama pull mistral


"""### Import model to Dspy pipeline"""

# Configure Ollama LLM with DSPy
ollama_llm = dspy.OllamaLocal(
    model="mistral",
    max_tokens=4000,
    timeout_s=4800,
)

# Configure DSPy with the Ollama LLM and Weaviate retriever
dspy.settings.configure(lm=ollama_llm, rm=retriever_model)

"""## Data preparation pipeline

## Fact-check + refine answer pipeline
"""

from datetime import datetime
# Define a signature for the summarization task
class SummarizeContext(dspy.Signature):
    __doc__ = """Summarize the key points from the context that are most relevant to answering the question."""
    context = dspy.InputField(desc="The context containing information relevant to answering the question.")
    summary = dspy.OutputField(desc="A concise summary of the context that extracts only the necessary information.")

# Define a signature for the answer generation task
class GenerateAnswer(dspy.Signature):
    __doc__ = """Generate a clear and precise answer that directly addresses the question using evidence from the context."""
    context = dspy.InputField(desc="The context from which to generate the answer.")
    question = dspy.InputField(desc="The question that needs to be answered.")
    answer = dspy.OutputField(desc="The final generated answer based on the context and question.")


class TimeAwareReranker:
    def __init__(self, date_format="%Y-%m-%d"):
        self.date_format = date_format

    def rerank(self, retrieval_results, date_field='date'):
        for result in retrieval_results:
            try:
                result['parsed_date'] = datetime.strptime(result[date_field], self.date_format)
            except (ValueError, KeyError):
                result['parsed_date'] = datetime.min

        reranked_results = sorted(retrieval_results, key=lambda x: x['parsed_date'], reverse=True)
        return reranked_results

class ClaimExtraction(dspy.Module):
    '''
    Extract claims from the provided response. A claim is a factual statement that can be verified or refuted based on evidence.
    Claims should be concise and to the point, representing specific assertions.
    Output each claim on a new line in a numbered list format.
    Only generate claims that are explicitly stated in the response. Do not include additional facts or information beyond the response.
    If the response is not a factual statement, please do not provide any claims.
    '''
    def __init__(self):
        super().__init__()
        self.prog = dspy.ChainOfThought("response -> claims")

    def forward(self, response):
        # Few-shot examples for the model
        examples = [
            {
                "response": "The Eiffel Tower is located in Paris, and it was built in 1889.",
                "claims": [
                    "1. The Eiffel Tower is located in Paris.",
                    "2. The Eiffel Tower was built in 1889."
                ]
            },
            {
                "response": "Tesla, Inc. was founded by Martin Eberhard and Marc Tarpenning in 2003, and Elon Musk later joined the company as an investor and chairman.",
                "claims": [
                    "1. Tesla, Inc. was founded by Martin Eberhard and Marc Tarpenning in 2003.",
                    "2. Elon Musk later joined Tesla as an investor and chairman."
                ]
            },
            {
                "response": "Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.",
                "claims": [
                    "1. Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976."
                ]
            }
        ]

        llm_output = self.prog(response=response, example=examples)

        return llm_output.claims


class ClaimClassification(dspy.Signature):
    """Label a claim as either 'Supported', 'Not Supported', or 'Insufficient Information' based on the evidence provided."""

    claims = dspy.InputField(desc="A list of claims to be classified.")
    evidence_passages = dspy.InputField(desc="Evidence supporting or refuting the claims.")
    additional_evidence = dspy.InputField(desc="Additional evidence that may support or refute the claims.")
    classifications = dspy.OutputField(desc="The classification of each claim.")

class DraftResponse(dspy.Signature):
    __doc__ = """
    Generate a clear, coherent response using the provided supported claims.
    """
    claims = dspy.InputField(desc="List of supported claims.")
    summary = dspy.InputField(desc="Summary of the overall response context.")
    question = dspy.InputField(desc="The question asked.")
    response = dspy.OutputField(desc="The generated response based on the supported claims.")

class RefineResponse(dspy.Signature):
    __doc__ = """
    Refine the provided draft response to improve its naturalness, relevance, coherence, and clarity.
    """
    draft_response = dspy.InputField(desc="The initial draft response that needs refinement.")
    question = dspy.InputField(desc="The question asked.")
    refined_response = dspy.OutputField(desc="The improved response after refinement.")

"""## Final RAG"""

class RAGPipeline(dspy.Module):
    def __init__(self, k=3):
        super().__init__()
        self.retrieve = ChromadbRM(
            collection_name="squad_data",
            persist_directory=db_path,
            embedding_function=m3_embedding_function,
            k=k
        )
        self.summarize = dspy.ChainOfThought(SummarizeContext)
        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)
        self.reranker = TimeAwareReranker(date_format="%Y-%m-%d")
        self.claim_extraction = ClaimExtraction()  # Claim extraction module
        self.draft_response = dspy.ChainOfThought(DraftResponse)  # Draft response module
        self.refine_response = dspy.ChainOfThought(RefineResponse)  # Refine response module
        self.claim_classification = dspy.ChainOfThought(ClaimClassification)  # Claim classification module

    def forward(self, question):
        # Step 1: Retrieve relevant contexts based on the question
        retrieval_results = self.retrieve(question)

        if not retrieval_results:
            return dspy.Prediction(
                context=[],
                initial_response="No relevant context found.",
                answer="No answer available due to lack of context.",
                question=question
            )

        # Step 2: Extract contexts and similarity scores
        contexts = [result['long_text'] for result in retrieval_results]
        similarity_scores = [result['score'] for result in retrieval_results]

        # Step 3: Summarize each context and combine summaries
        summarized_contexts = [
            f"• {self.summarize(context=ctx).summary} (Similarity: {score:.4f})"
            for ctx, score in zip(contexts, similarity_scores)
        ]

        # Step 4: Generate an answer from the summarized context
        answer = self.generate_answer(context="\n".join(summarized_contexts), question=question).answer

        # Step 5: Extract claims from the generated answer
        extracted_claims = self.claim_extraction(response=answer)

        # Step 6: Classify the claims based on the retrieved passages (not additional evidence)
        classification_result = self.claim_classification(
            claims="\n".join(extracted_claims),
            evidence_passages="\n".join(contexts),  # Use retrieved passages here
            additional_evidence=""  # No additional evidence needed
        ).classifications

        # Step 7: Filter the supported claims for the draft response
        supported_claims = [claim for claim, classification in zip(extracted_claims, classification_result) if "Supported" in classification]
        supported_claims_str = "\n".join(supported_claims)

        # Step 8: Generate the draft response using the supported claims
        draft_response = self.draft_response(
            claims=supported_claims_str,
            summary="\n".join(summarized_contexts),
            question=question
        ).response

        # Step 9: Refine the draft response for clarity and coherence
        refined_response = self.refine_response(
            draft_response=draft_response,
            question=question
        ).refined_response

        # Return the final prediction with refined response
        return dspy.Prediction(
            golden_context=summarized_contexts,
            summary="\n".join(summarized_contexts),
            golden_answer=answer,
            claims=extracted_claims,
            context=contexts,  # Use retrieved passages as context
            draft_response=draft_response,  # Include draft response
            answer=refined_response,  # Include refined response
            question=question,
        )

# Test the RAGPipeline with a question
rag_pipeline = RAGPipeline(k=2)
result = rag_pipeline(question="What is another name for Universal Studios Inc.?")
print(result)

rag_pipeline = RAGPipeline(k=2)
result = rag_pipeline(question="Where was the Universal Manufacturing Film Company incorporated?")
print(result)

"""## Evaluation pipeline"""



from sklearn.metrics import precision_score, recall_score, f1_score
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer


def compute_precision_recall_f1(golden_answer, generated_answer):
    golden_tokens = set(golden_answer.split())
    generated_tokens = set(generated_answer.split())

    true_positives = len(golden_tokens & generated_tokens)
    false_positives = len(generated_tokens - golden_tokens)
    false_negatives = len(golden_tokens - generated_tokens)

    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1


def compute_rouge(golden_answer, generated_answer):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    scores = scorer.score(golden_answer, generated_answer)

    rouge_scores = {
        "rouge-1": scores["rouge1"].fmeasure,
        "rouge-2": scores["rouge2"].fmeasure,
        "rouge-L": scores["rougeL"].fmeasure
    }

    return rouge_scores


def compute_bleu(golden_answer, generated_answer):
    golden_tokens = [golden_answer.split()]
    generated_tokens = generated_answer.split()

    bleu_score = sentence_bleu(golden_tokens, generated_tokens)
    return bleu_score

class RAGEvaluation(dspy.Signature):
    __doc__ = "Evaluate the RAG pipeline's generated answers using standard metrics."
    golden_answer = dspy.InputField(desc="The reference (ground truth) answer.")
    generated_answer = dspy.InputField(desc="The answer generated by the pipeline.")
    precision = dspy.OutputField(desc="Precision of the generated answer.")
    recall = dspy.OutputField(desc="Recall of the generated answer.")
    f1_score = dspy.OutputField(desc="F1-score of the generated answer.")
    rouge = dspy.OutputField(desc="ROUGE score of the generated answer.")
    bleu = dspy.OutputField(desc="BLEU score of the generated answer.")

class EvaluatePipeline(dspy.Module):
    def __init__(self):
        super().__init__()
        self.prog = dspy.ChainOfThought(RAGEvaluation, max_depth=10)

    def forward(self, golden_answer, generated_answer):
        try:
            print("\n=== Starting Evaluation ===")
            print(f"Golden Answer: {golden_answer}")
            print(f"Generated Answer: {generated_answer}")

            # Validate inputs
            if not golden_answer or not generated_answer:
                print("Invalid inputs: One or both answers are empty.")
                raise AssertionError("Empty input values for evaluation.")

            # Compute precision, recall, F1-score
            precision, recall, f1 = compute_precision_recall_f1(golden_answer, generated_answer)

            # Compute ROUGE scores
            rouge_scores = compute_rouge(golden_answer, generated_answer)

            # Compute BLEU score
            bleu_score = compute_bleu(golden_answer, generated_answer)

            print(f"\n=== Metrics ===")
            print(f"Precision: {precision}")
            print(f"Recall: {recall}")
            print(f"F1-Score: {f1}")
            print(f"ROUGE Scores: {rouge_scores}")
            print(f"BLEU Score: {bleu_score}")

            # Return the computed metrics
            return {
                "precision": precision,
                "recall": recall,
                "f1_score": f1,
                "rouge": rouge_scores,
                "bleu": bleu_score
            }

        except Exception as e:
            print("\nError occurred during evaluation:", e)
            print("Using fallback metrics.")
            return {
                "precision": 0.0,
                "recall": 0.0,
                "f1_score": 0.0,
                "rouge": {"rouge-1": 0.0, "rouge-2": 0.0, "rouge-L": 0.0},
                "bleu": 0.0
            }

rag_pipeline = RAGPipeline(k=2)

question = "Where was the Universal Manufacturing Film Company incorporated?"

# Run the RAG pipeline
result = rag_pipeline(question=question)

evaluation_module = EvaluatePipeline()
metrics = evaluation_module.forward(
    golden_answer=result.golden_answer,
    generated_answer=result.answer
)
print(metrics)